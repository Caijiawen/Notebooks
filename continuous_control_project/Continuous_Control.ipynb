{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "You are welcome to use this coding environment to train your agent for the project.  Follow the instructions below to get started!\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "Run the next code cell to install a few packages.  This line will take a few minutes to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install ./python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environments corresponding to both versions of the environment are already saved in the Workspace and can be accessed at the file paths provided below.  \n",
    "\n",
    "Please select one of the two options below for loading the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_size -> 5.0\n",
      "\t\tgoal_speed -> 1.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "# select this option to load version 1 (with a single agent) of the environment\n",
    "# env = UnityEnvironment(file_name='/data/Reacher_One_Linux_NoVis/Reacher_One_Linux_NoVis.x86_64')\n",
    "\n",
    "# select this option to load version 2 (with 20 agents) of the environment\n",
    "env = UnityEnvironment(file_name='/data/Reacher_Linux_NoVis/Reacher.x86_64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Size of each action: 4\n",
      "There are 20 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [  0.00000000e+00  -4.00000000e+00   0.00000000e+00   1.00000000e+00\n",
      "  -0.00000000e+00  -0.00000000e+00  -4.37113883e-08   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00  -1.00000000e+01   0.00000000e+00\n",
      "   1.00000000e+00  -0.00000000e+00  -0.00000000e+00  -4.37113883e-08\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   5.75471878e+00  -1.00000000e+00\n",
      "   5.55726624e+00   0.00000000e+00   1.00000000e+00   0.00000000e+00\n",
      "  -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Note that **in this coding environment, you will not be able to watch the agents while they are training**, and you should set `train_mode=True` to restart the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tick 1001\n",
      "Total score (averaged over agents) this episode: 0.13699999693781137\n",
      "Tick 1001\n",
      "Total score (averaged over agents) this episode: 0.0789999982342124\n",
      "Tick 1001\n",
      "Total score (averaged over agents) this episode: 0.09549999786540866\n",
      "Tick 1001\n",
      "Total score (averaged over agents) this episode: 0.07849999824538827\n",
      "Tick 1001\n",
      "Total score (averaged over agents) this episode: 0.1579999964684248\n",
      "Tick 1001\n",
      "Total score (averaged over agents) this episode: 0.12649999717250465\n",
      "Tick 1001\n",
      "Total score (averaged over agents) this episode: 0.13449999699369072\n",
      "Tick 1001\n",
      "Total score (averaged over agents) this episode: 0.16449999632313847\n",
      "Tick 1001\n",
      "Total score (averaged over agents) this episode: 0.20199999548494815\n",
      "Tick 1001\n",
      "Total score (averaged over agents) this episode: 0.06449999855831265\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    env_info = env.reset(train_mode=True)[brain_name]      # reset the environment    \n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    scores = np.zeros(num_agents)  # initialize the score (for each agent)\n",
    "    tick = 0\n",
    "    while True:\n",
    "        tick += 1\n",
    "        actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "        actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "        env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        scores += env_info.rewards                         # update the score (for each agent)\n",
    "        states = next_states                               # roll over states to next time step\n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "    print(f'Tick {tick}')\n",
    "    print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO Solution part1: Neural Network Model\n",
    "\n",
    "In the network model part , we use 2 networks : actor network and critic network.\n",
    "\n",
    "### Actor network use state to predict action.\n",
    "\n",
    "\n",
    "network architecture:\n",
    "\n",
    "- Actor(\n",
    "  - (fc1): Linear(in_features=33, out_features=512, bias=True)\n",
    "  - (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
    "  - (fc3): Linear(in_features=256, out_features=4, bias=True)\n",
    "- )\n",
    "\n",
    "### Critic network use state to predict value(expectation of discounted return of future).\n",
    "\n",
    "network architecture:\n",
    "\n",
    "- Critic(\n",
    "  - (fcs1): Linear(in_features=33, out_features=512, bias=True)\n",
    "  - (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
    "  - (fc3): Linear(in_features=256, out_features=1, bias=True)\n",
    "- )\n",
    "\n",
    "### Policy network to synthesis information in actor network and critic network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def hidden_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return (-lim, lim)\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    \"\"\"(Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed=0, fc1_units=512 , fc2_units = 256):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(Policy, self).__init__()\n",
    "        print(\"network state size: \" , state_size)\n",
    "        print(\"network action size: \" , action_size)\n",
    "        self.actor = Actor(state_size , action_size , seed,fc1_units , fc2_units)\n",
    "        self.critic = Critic(state_size , action_size , seed , fc1_units , fc2_units)\n",
    "        self.std = torch.Tensor(nn.Parameter(torch.ones(1, action_size)))\n",
    "\n",
    "        \n",
    "    \n",
    "    def forward(self, state, action = None):\n",
    "        \"\"\"Build an actor (policy) network that maps states -> actions.\"\"\"\n",
    "        \n",
    "        \n",
    "        phi_a = self.actor(state)\n",
    "        phi_v = self.critic(state)\n",
    "        \n",
    "        dist = torch.distributions.Normal(phi_a, self.std)\n",
    "        if action is None:\n",
    "            action = dist.sample()\n",
    "        log_prob = dist.log_prob(action).sum(-1).unsqueeze(-1)\n",
    "        entropy = dist.entropy()\n",
    "        \n",
    "        return {'a': action,\n",
    "                'log_pi_a': log_prob,\n",
    "                'ent': entropy,\n",
    "                'mean': phi_a,\n",
    "                'v': phi_v}\n",
    "        \n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_size , action_size , seed, fc1_units , fc2_units):\n",
    "        super(Actor, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3,3e-3)\n",
    "    \n",
    "    def forward(self,state):\n",
    "        xs = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(xs))\n",
    "        action = F.tanh(self.fc3(x))\n",
    "        return action\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_size , action_size , seed , fc1_units , fc2_units):\n",
    "        super(Critic, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, 1)\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3,3e-3)\n",
    "        \n",
    "    def forward(self,state):\n",
    "        xs = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(xs))\n",
    "        value = self.fc3(x)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO Solution part2: PPO agent\n",
    "\n",
    "In the agent part , we process data and train agent to get higher score in every episode. We divide every step into three parts: collect trajectories , compute advantages and training.\n",
    "\n",
    "### Collect trajectories\n",
    "\n",
    "\n",
    "First we reset the environment , use latest policy to compute action , gaussian probability and value.\n",
    "then we use action to get next state , reward and done information.\n",
    "We collect every step's information into list at last.\n",
    "\n",
    "\n",
    "### Compute advantages\n",
    "\n",
    "In this part, we use GAE(generalized advantage function) as advantage function. In every time step , the gae equals td error plus gae tau multiply next time step's advantage function.\n",
    "\n",
    "\n",
    "### Training\n",
    "\n",
    "We use stochastic gradient descent to train the networks.\n",
    "To avoid \"cliff\" in training process , we use clip function to control the value of loss.\n",
    "\n",
    "\n",
    "### Hyperparameters\n",
    "\n",
    "- SGD_epochs : 4 \n",
    "- SGD_batch_size : 64 \n",
    "- discount rate: 0.99\n",
    "- ratio clip(clip function parameter): 0.1\n",
    "- ratio clip decay rate(ratio clip parameter decay after every episode): 0.999\n",
    "- GAE tau: 0.95\n",
    "- gradient clip parameter: 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ppo_model import Policy\n",
    "from collections import deque\n",
    "\n",
    "class Config:\n",
    "    optimizer_fn = lambda params: torch.optim.Adam(params, 1e-4, eps=1e-5)\n",
    "    optimization_epochs = 4\n",
    "    network = lambda state_size , action_size: Policy(state_size , action_size)\n",
    "    discount = 0.99\n",
    "    ratio_clip = 0.1\n",
    "    batch_size = 64\n",
    "    entropy_weight = 0.01\n",
    "    max_episode = 500\n",
    "    gradient_clip = 5\n",
    "    use_gae = True\n",
    "    gae_tau = 0.95\n",
    "\n",
    "\n",
    "def random_sample(indices, batch_size):\n",
    "        indices = np.asarray(np.random.permutation(indices))\n",
    "        batches = indices[:len(indices) // batch_size * batch_size].reshape(-1, batch_size)\n",
    "        for batch in batches:\n",
    "            yield batch\n",
    "        r = len(indices) % batch_size\n",
    "        if r:\n",
    "            yield indices[-r:]\n",
    "        \n",
    "def to_tt(np_array):\n",
    "    return torch.tensor(np_array , dtype = torch.float , device=device)\n",
    "        \n",
    "class PPOAgent(object):\n",
    "    def __init__(self, env):\n",
    "        \n",
    "        self.env = env\n",
    "        self.brain_name = self.env.brain_names[0]\n",
    "        self.brain = self.env.brains[self.brain_name]\n",
    "        self.env_info = self.env.reset(train_mode=True)[self.brain_name]\n",
    "        self.num_agents = len(self.env_info.agents)\n",
    "        print('Number of agents:', self.num_agents)\n",
    "        self.action_size = self.brain.vector_action_space_size\n",
    "        print('Size of each action:', self.action_size)\n",
    "        self.states = self.env_info.vector_observations\n",
    "        self.state_size = self.states.shape[1]\n",
    "        print('There are {} agents. Each observes a state with length: {}'.format(self.states.shape[0], self.state_size))\n",
    "        \n",
    "        self.network = Config.network(self.state_size , self.action_size)\n",
    "        \n",
    "        self.opt = Config.optimizer_fn(self.network.parameters())\n",
    "        \n",
    "        self.total_steps = 0\n",
    "        \n",
    "        \n",
    "        self.score = None\n",
    "        self.score_window = deque(maxlen = 100)\n",
    "        self.score_list = []\n",
    "    \n",
    "    def collect_trajectories_vec(self):\n",
    "        \n",
    "        state_list=[]\n",
    "        reward_list=[]\n",
    "        prob_list=[]\n",
    "        action_list=[]\n",
    "        dones_list=[]\n",
    "        values_list = []\n",
    "\n",
    "        self.env_info = self.env.reset(train_mode=True)[self.brain_name]\n",
    "        \n",
    "        states = self.env_info.vector_observations                  # get the current state (for each agent)\n",
    "        scores = np.zeros(self.num_agents)                          # initialize the score (for each agent)\n",
    "        \n",
    "        while True:\n",
    "            ### Important note: Don't add next_state to state_list!!!\n",
    "            state_list.append(states)\n",
    "            \n",
    "            probs = []\n",
    "            values = []\n",
    "            states = torch.tensor(states ,dtype = torch.float ,device = device)\n",
    "            prediction = self.network(states)\n",
    "            \n",
    "            self.network.eval()\n",
    "            with torch.no_grad():\n",
    "                action = prediction['a'].squeeze().cpu().detach().numpy()\n",
    "                prob = prediction['log_pi_a'].squeeze(-1).cpu().detach().numpy()\n",
    "                value = prediction['v'].squeeze(-1).cpu().detach().numpy()\n",
    "            self.network.train()\n",
    "            \n",
    "            self.env_info = self.env.step(action)[brain_name]           # send all actions to tne environment\n",
    "            next_states = self.env_info.vector_observations         # get next state (for each agent)\n",
    "            rewards = self.env_info.rewards                         # get reward (for each agent)\n",
    "            dones = self.env_info.local_done                        # see if episode finished\n",
    "            scores += self.env_info.rewards                         # update the score (for each agent)\n",
    "            states = next_states                               # roll over states to next time step\n",
    "            \n",
    "            \n",
    "            reward_list.append(rewards)\n",
    "            dones_list.append(dones)\n",
    "            \n",
    "            action_list.append(action)\n",
    "            prob_list.append(prob)\n",
    "            values_list.append(value)\n",
    "        \n",
    "            if np.any(dones):                                  # exit loop if episode finished\n",
    "                break\n",
    "#         print(\"scores: \" , scores)\n",
    "#         print(\"mean score: \" , np.mean(scores))\n",
    "        \n",
    "        self.score = np.mean(scores)\n",
    "        self.score_window.append(self.score)\n",
    "        self.score_list.append(self.score)\n",
    "        return state_list, reward_list , action_list, prob_list, dones_list, values_list\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    def compute_advantage(self,r,d,v):\n",
    "        length = len(v)\n",
    "        \n",
    "        ## The order in advantages_list should be keeped!!\n",
    "        advantages_list = [None]*length\n",
    "        advantages = to_tt(np.zeros(self.num_agents))\n",
    "        returns = to_tt(np.zeros(self.num_agents))\n",
    "        for i in reversed(range(length)):\n",
    "            done = np.array(d[i]).astype(int)\n",
    "            done = to_tt(done)\n",
    "            returns = to_tt(r[i])\n",
    "            next_v = to_tt(v[min(length-1,i+1)])\n",
    "            this_v = to_tt(v[i])\n",
    "            td_error = returns + Config.discount * (1-done) * next_v.detach() - to_tt(v[i]).detach()\n",
    "                \n",
    "            if not Config.use_gae:\n",
    "                advantages = td_error\n",
    "            else:\n",
    "                advantages = Config.gae_tau * Config.discount * advantages * (1-done) + td_error\n",
    "            advantages_list[i] = advantages\n",
    "        return advantages_list\n",
    "        \n",
    "    \n",
    "    \n",
    "    def mystep(self):\n",
    "        #20个agent是分开训练还是一起训练？\n",
    "        s,r,a,p,d,v = self.collect_trajectories_vec()\n",
    "        s,r,a,p,d,v = to_tt(s),to_tt(r),to_tt(a).detach(),to_tt(p).detach(),to_tt(d),to_tt(v)\n",
    "        \n",
    "#         print(\"s0\",s[0]) # 33*20\n",
    "#         print(\"r0\",r[0]) # 1*20\n",
    "#         print(\"a0\",a[0]) # 4*20\n",
    "#         print(\"p0\",p[0]) # 1*20\n",
    "#         print(\"d0\",d[0]) # 1*20\n",
    "#         print(\"v0\",v[0]) # 1*20\n",
    "#         print(\"s-1\",s[-1]) # 33*20\n",
    "#         print(\"r-1\",r[-1]) # 1*20\n",
    "#         print(\"a-1\",a[-1]) # 4*20\n",
    "#         print(\"p-1\",p[-1]) # 1*20\n",
    "#         print(\"d-1\",d[-1]) # 1*20\n",
    "#         print(\"v-1\",v[-1]) # 1*20\n",
    "        \n",
    "        \n",
    "        for _ in range(Config.optimization_epochs):\n",
    "            advantages = self.compute_advantage(r,d,v)\n",
    "\n",
    "#             print(\"advantage0\" , advantages[0])\n",
    "#             print(\"advantage-1\" , advantages[-1])\n",
    "\n",
    "            advantages = torch.stack(advantages)# .squeeze(2)\n",
    "            advantages = (advantages - advantages.mean()) / advantages.std()\n",
    "\n",
    "        \n",
    "            sampler = random_sample(np.arange(advantages.shape[0]) , Config.batch_size)\n",
    "            for batch_indices in sampler:\n",
    "                sampled_states = to_tt(s[batch_indices])\n",
    "                sampled_action = to_tt(a[batch_indices])\n",
    "                sampled_log_probs_old = to_tt(p[batch_indices])\n",
    "                sampled_returns = to_tt(r[batch_indices])\n",
    "                sampled_advantages = to_tt(advantages[batch_indices])\n",
    "                prediction = self.network(sampled_states , sampled_action)\n",
    "                ratio = (prediction['log_pi_a'].squeeze() - sampled_log_probs_old).exp()\n",
    "                \n",
    "                obj = ratio * sampled_advantages\n",
    "                obj_clipped = ratio.clamp(1.0-Config.ratio_clip , 1.0+Config.ratio_clip) * sampled_advantages\n",
    "                \n",
    "                    \n",
    "                policy_loss = -torch.min(obj , obj_clipped).mean() #- Config.entropy_weight * prediction['ent'].mean()\n",
    "                value_loss = 0.5 * (sampled_returns - prediction['v'].squeeze()).pow(2).mean()\n",
    "\n",
    "                self.opt.zero_grad()\n",
    "                policy_loss.backward()\n",
    "#                 (policy_loss + value_loss).backward()\n",
    "                nn.utils.clip_grad_norm_(self.network.parameters(), Config.gradient_clip)\n",
    "                self.opt.step()\n",
    "\n",
    "    def main(self):\n",
    "#         !pip install progressbar\n",
    "#         import progressbar as pb\n",
    "#         widget = ['training loop: ', pb.Percentage(), ' ', pb.Bar(), ' ', pb.ETA() ]\n",
    "#         timer = pb.ProgressBar(widgets=widget, maxval=Config.max_episode).start()\n",
    "        for e in range(Config.max_episode):\n",
    "            Config.entropy_weight *= 0.995\n",
    "            Config.ratio_clip *= 0.999\n",
    "            self.mystep()\n",
    "            \n",
    "            msg = '\\rEpisode {}\\t Average Score: {:.2f}\\t Score: {:.2f}'\n",
    "            print(msg.format(e, np.mean(self.score_window), self.score), end=\"\")\n",
    "            if e % 10 == 0:\n",
    "                print(msg.format(e, np.mean(self.score_window), self.score))\n",
    "                \n",
    "            if np.mean(self.score_window) >= 30.:\n",
    "                print(f\"Mission Complete!!!  in {e} Episode\")\n",
    "                break\n",
    "#             timer.update(e+1)\n",
    "#         timer.finish()\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Size of each action: 4\n",
      "There are 20 agents. Each observes a state with length: 33\n",
      "network state size:  33\n",
      "network action size:  4\n",
      "Episode 0\t Average Score: 0.04\t Score: 0.04\n",
      "Episode 10\t Average Score: 0.40\t Score: 0.86\n",
      "Episode 20\t Average Score: 0.95\t Score: 1.74\n",
      "Episode 30\t Average Score: 1.46\t Score: 2.73\n",
      "Episode 40\t Average Score: 1.94\t Score: 4.34\n",
      "Episode 50\t Average Score: 2.45\t Score: 5.00\n",
      "Episode 60\t Average Score: 2.99\t Score: 6.20\n",
      "Episode 70\t Average Score: 3.57\t Score: 7.49\n",
      "Episode 80\t Average Score: 4.20\t Score: 8.90\n",
      "Episode 90\t Average Score: 4.80\t Score: 10.76\n",
      "Episode 100\t Average Score: 5.54\t Score: 11.69\n",
      "Episode 110\t Average Score: 6.91\t Score: 16.10\n",
      "Episode 120\t Average Score: 8.47\t Score: 19.30\n",
      "Episode 130\t Average Score: 10.15\t Score: 21.90\n",
      "Episode 140\t Average Score: 12.06\t Score: 26.03\n",
      "Episode 150\t Average Score: 14.14\t Score: 25.17\n",
      "Episode 160\t Average Score: 16.29\t Score: 28.66\n",
      "Episode 170\t Average Score: 18.49\t Score: 28.88\n",
      "Episode 180\t Average Score: 20.74\t Score: 32.97\n",
      "Episode 190\t Average Score: 22.99\t Score: 32.85\n",
      "Episode 200\t Average Score: 25.23\t Score: 35.21\n",
      "Episode 210\t Average Score: 27.33\t Score: 35.63\n",
      "Episode 220\t Average Score: 29.15\t Score: 35.83\n",
      "Episode 226\t Average Score: 30.16\t Score: 35.49Mission Complete!!!  in 226 Episode\n"
     ]
    }
   ],
   "source": [
    "# from ppo_agent import PPOAgent\n",
    "agent = PPOAgent(env)\n",
    "agent.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f4508f3cf98>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f450d079c88>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(agent.score_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model Weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent.network.actor.state_dict(), 'actor.pth')\n",
    "torch.save(agent.network.critic.state_dict(), 'critic.pth')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
